"""
Groq-Optimized Hierarchical Transformer

Uses hierarchical chunking to optimize Groq transformations:
1. Process base chunks (300 tokens each) with Groq
2. Build up summary levels progressively
3. Maintain quantum state coherence across levels
4. Recombine into full transformed text
"""

import asyncio
import logging
import time
from typing import Dict, List, Tuple, Optional
import requests
import numpy as np

from .hierarchical_chunking import (
    HierarchicalChunker, 
    ChunkTree, 
    TextChunk,
    create_hierarchical_transformation_plan
)

logger = logging.getLogger(__name__)

class GroqHierarchicalTransformer:
    """Hierarchical transformer optimized for Groq's strengths."""
    
    def __init__(self, 
                 api_base_url: str = "http://localhost:8192",
                 groq_api_key: str = None,
                 max_concurrent_requests: int = 3):
        self.api_base_url = api_base_url
        self.groq_api_key = groq_api_key
        self.max_concurrent_requests = max_concurrent_requests
        self.chunker = HierarchicalChunker()
    
    def create_groq_prompt(self, 
                          chunk: TextChunk, 
                          transformation_request: Dict,
                          context: Dict) -> str:
        """Create optimized prompt for Groq based on chunk level and context."""
        
        if chunk.level == 0:
            # Base chunk - detailed transformation
            return self._create_base_chunk_prompt(chunk, transformation_request, context)
        else:
            # Summary chunk - coherence and flow
            return self._create_summary_chunk_prompt(chunk, transformation_request, context)
    
    def _create_base_chunk_prompt(self, 
                                 chunk: TextChunk, 
                                 transformation_request: Dict,
                                 context: Dict) -> str:
        """Create prompt for base-level chunks."""
        transformation_type = transformation_request.get('transformation_name', 'enhance_narrative')
        strength = transformation_request.get('strength', 0.7)
        
        # Context awareness
        position_context = ""
        if context.get('structure_info', {}).get('chunk_index', 0) == 0:
            position_context = "This is the opening section. "
        elif context.get('next_chunk') is None:
            position_context = "This is the concluding section. "
        else:
            position_context = "This is a middle section. "
        
        overlap_context = ""
        if context.get('structure_info', {}).get('has_overlap_before'):
            overlap_context += "The beginning may overlap with previous content. "
        if context.get('structure_info', {}).get('has_overlap_after'):
            overlap_context += "The ending may overlap with following content. "
        
        prompt = f"""Transform this text segment using the "{transformation_type}" style.

CONTEXT: {position_context}{overlap_context}This is part {context.get('structure_info', {}).get('chunk_index', 0) + 1} of {context.get('structure_info', {}).get('total_chunks_at_level', 1)} segments.

REQUIREMENTS:
- Apply {transformation_type} transformation with {strength:.1f} strength
- Maintain semantic coherence and flow
- Preserve key information and concepts
- Keep natural transitions for segment boundaries
- Output ONLY the transformed text, no explanations

TEXT SEGMENT:
{chunk.text}

TRANSFORMED VERSION:"""
        
        return prompt
    
    def _create_summary_chunk_prompt(self, 
                                   chunk: TextChunk, 
                                   transformation_request: Dict,
                                   context: Dict) -> str:
        """Create prompt for summary-level chunks."""
        transformation_type = transformation_request.get('transformation_name', 'enhance_narrative')
        
        parent_count = len(chunk.parent_chunks) if chunk.parent_chunks else 0
        
        prompt = f"""Synthesize and refine this multi-segment text using the "{transformation_type}" style.

CONTEXT: This combines {parent_count} previously transformed segments into a coherent whole.

REQUIREMENTS:
- Unify the segments into flowing, coherent text
- Smooth transitions between sections
- Maintain the {transformation_type} style throughout
- Preserve all key information and concepts
- Remove redundancy from overlapping segments
- Output ONLY the unified text, no explanations

SEGMENTS TO UNIFY:
{chunk.text}

UNIFIED VERSION:"""
        
        return prompt
    
    async def transform_chunk_with_groq(self, 
                                       chunk: TextChunk, 
                                       transformation_request: Dict,
                                       context: Dict) -> Tuple[str, Dict]:
        """Transform a single chunk using Groq."""
        start_time = time.time()
        
        if not self.groq_api_key:
            raise ValueError("Groq API key required for hierarchical transformation")
        
        prompt = self.create_groq_prompt(chunk, transformation_request, context)
        
        groq_request = {
            "model": "openai/gpt-oss-20b",
            "messages": [
                {
                    "role": "system",
                    "content": "You are a precise text transformation engine. Output only the transformed text with no additional commentary, explanations, or formatting."
                },
                {
                    "role": "user", 
                    "content": prompt
                }
            ],
            "temperature": transformation_request.get('creativity_level', 0.3),
            "max_tokens": min(2000, chunk.token_count * 2),  # Allow some expansion
            "top_p": 0.9,
            "stop": ["\n\n---", "\n\n*", "EXPLANATION:", "ANALYSIS:"]
        }
        
        try:
            response = requests.post(
                "https://api.groq.com/openai/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.groq_api_key}",
                    "Content-Type": "application/json"
                },
                json=groq_request,
                timeout=60
            )
            
            duration = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                raw_content = result["choices"][0]["message"]["content"].strip()
                
                # Handle thinking model output - extract thinking content for debugging
                thinking_content = ""
                transformed_text = raw_content
                
                # Extract thinking content (gpt-oss models use <think>...</think> tags)
                import re
                think_pattern = r'<think>(.*?)</think>'
                think_matches = re.findall(think_pattern, raw_content, re.DOTALL)
                if think_matches:
                    thinking_content = "\n".join(think_matches)
                    # Remove thinking tags from output
                    transformed_text = re.sub(think_pattern, '', raw_content, flags=re.DOTALL).strip()
                    
                    # Log thinking content for debugging
                    logger.debug(f"Chunk {chunk.id} thinking: {thinking_content[:200]}...")
                
                # Clean up the response
                if "TRANSFORMED VERSION:" in transformed_text:
                    transformed_text = transformed_text.split("TRANSFORMED VERSION:")[-1].strip()
                
                if "UNIFIED VERSION:" in transformed_text:
                    transformed_text = transformed_text.split("UNIFIED VERSION:")[-1].strip()
                
                # Remove common wrapper patterns
                transformed_text = transformed_text.strip('"\' \n\r\t')
                
                audit_info = {
                    "chunk_id": chunk.id,
                    "chunk_level": chunk.level,
                    "provider": "groq", 
                    "model": "openai/gpt-oss-20b",
                    "duration": duration,
                    "success": True,
                    "original_tokens": chunk.token_count,
                    "transformed_length": len(transformed_text),
                    "prompt_type": "base_chunk" if chunk.level == 0 else "summary_chunk",
                    "thinking_content": thinking_content,  # Preserve thinking for debugging
                    "has_thinking": bool(thinking_content)
                }
                
                logger.info(f"✅ Groq transformed chunk {chunk.id}: {chunk.token_count} tokens → {len(transformed_text)} chars ({duration:.2f}s)")
                
                return transformed_text, audit_info
            
            else:
                error_msg = f"Groq API error: {response.status_code}"
                logger.error(f"❌ {error_msg} for chunk {chunk.id}")
                
                return None, {
                    "chunk_id": chunk.id,
                    "provider": "groq",
                    "duration": duration,
                    "success": False,
                    "error": error_msg
                }
                
        except Exception as e:
            duration = time.time() - start_time
            error_msg = f"Groq request failed: {str(e)}"
            logger.error(f"❌ {error_msg} for chunk {chunk.id}")
            
            return None, {
                "chunk_id": chunk.id,
                "provider": "groq", 
                "duration": duration,
                "success": False,
                "error": error_msg
            }
    
    async def process_chunk_level(self, 
                                 chunk_ids: List[str], 
                                 tree: ChunkTree,
                                 transformation_request: Dict,
                                 transformed_chunks: Dict[str, str]) -> Tuple[Dict[str, str], List[Dict]]:
        """Process all chunks at a given level concurrently."""
        level = tree.chunks[chunk_ids[0]].level
        logger.info(f"🔄 Processing level {level} with {len(chunk_ids)} chunks")
        
        results = {}
        audit_info = []
        
        # Create semaphore to limit concurrent requests
        semaphore = asyncio.Semaphore(self.max_concurrent_requests)
        
        async def process_single_chunk(chunk_id: str):
            async with semaphore:
                chunk = tree.chunks[chunk_id]
                
                # For summary chunks, update text with transformed parent content
                if chunk.level > 0 and chunk.parent_chunks:
                    parent_texts = []
                    for parent_id in chunk.parent_chunks:
                        if parent_id in transformed_chunks:
                            parent_texts.append(transformed_chunks[parent_id])
                        else:
                            # Fallback to original text
                            parent_texts.append(tree.chunks[parent_id].text)
                    
                    # Update chunk text with transformed parent content
                    chunk.text = "\n\n".join(parent_texts)
                
                context = self.chunker.get_chunk_context(chunk_id, tree)
                transformed_text, chunk_audit = await self.transform_chunk_with_groq(
                    chunk, transformation_request, context
                )
                
                return chunk_id, transformed_text, chunk_audit
        
        # Process chunks concurrently
        tasks = [process_single_chunk(chunk_id) for chunk_id in chunk_ids]
        chunk_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Collect results
        for result in chunk_results:
            if isinstance(result, Exception):
                logger.error(f"Chunk processing failed: {result}")
                continue
            
            chunk_id, transformed_text, chunk_audit = result
            
            if transformed_text:
                results[chunk_id] = transformed_text
                transformed_chunks[chunk_id] = transformed_text
            
            audit_info.append(chunk_audit)
        
        logger.info(f"✅ Completed level {level}: {len(results)}/{len(chunk_ids)} chunks successful")
        return results, audit_info
    
    async def transform_hierarchically(self, 
                                     text: str, 
                                     transformation_request: Dict) -> Tuple[str, Dict]:
        """Perform hierarchical transformation using Groq."""
        start_time = time.time()
        
        # Create transformation plan
        plan = create_hierarchical_transformation_plan(text, transformation_request)
        tree = plan["tree"]
        processing_order = plan["processing_order"]
        
        logger.info(f"🚀 Starting hierarchical transformation: {plan['total_chunks']} chunks across {plan['levels']} levels")
        
        # Track all transformations and audit info
        transformed_chunks = {}
        all_audit_info = []
        
        # Process each level bottom-up
        for level_chunks in processing_order:
            level_results, level_audit = await self.process_chunk_level(
                level_chunks, tree, transformation_request, transformed_chunks
            )
            all_audit_info.extend(level_audit)
        
        # Merge final result
        final_text = self.chunker.merge_transformed_chunks(tree, transformed_chunks)
        
        total_duration = time.time() - start_time
        
        # Create comprehensive audit trail
        audit_trail = {
            "transformation_strategy": "groq_hierarchical",
            "chunk_tree_info": {
                "total_chunks": plan['total_chunks'],
                "levels": plan['levels'],
                "base_chunks": len(tree.levels[0]),
                "estimated_groq_calls": plan['estimated_groq_calls']
            },
            "processing_summary": {
                "total_duration": total_duration,
                "successful_chunks": len([a for a in all_audit_info if a.get('success')]),
                "failed_chunks": len([a for a in all_audit_info if not a.get('success')]),
                "average_chunk_duration": sum(a.get('duration', 0) for a in all_audit_info) / len(all_audit_info) if all_audit_info else 0
            },
            "chunk_details": all_audit_info,
            "final_result": {
                "original_length": len(text),
                "transformed_length": len(final_text),
                "compression_ratio": len(final_text) / len(text) if text else 1.0
            }
        }
        
        logger.info(f"🎉 Hierarchical transformation complete: {len(text)} → {len(final_text)} chars in {total_duration:.2f}s")
        
        return final_text, audit_trail

# Async wrapper for integration with existing sync code
def transform_text_hierarchically(text: str, 
                                transformation_request: Dict, 
                                groq_api_key: str) -> Tuple[str, Dict]:
    """Synchronous wrapper for hierarchical transformation."""
    transformer = GroqHierarchicalTransformer(groq_api_key=groq_api_key)
    
    # Run in async context
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    
    try:
        result = loop.run_until_complete(
            transformer.transform_hierarchically(text, transformation_request)
        )
        return result
    finally:
        loop.close()